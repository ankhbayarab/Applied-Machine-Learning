{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Programming Exercise 1: Linear Regression - Optional\n",
    "                                      \n",
    "\n",
    "If you have successfully completed the required part of Exercise 1, congratulations!\n",
    "\n",
    "You now understand linear regression and should be able to start using it\n",
    "on your own datasets. For the rest of this programming exercise, we have\n",
    "included a few functions you will have to implement. These exercises will help you\n",
    "gain a deeper understanding of the material, and if you are able to do\n",
    "so, we encourage you to complete them as well. In this notebook, you will implement linear regression on multiple variables and get to see it work on data. You will be asked to complete the following functions:\n",
    "\n",
    "-  computeCostMulti - Cost function for multiple variables\n",
    "-  gradientDescentMulti - Gradient descent function for multiple variables\n",
    "-  featureNormalize - Function to normalize features\n",
    "-  normalEqn - Function to compute the normal equations\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "For each function, we have provided starter code for you. You will find a description of the problems followed by a cell which contains some code. You will have to write your own code in these cells to complete the four functions mentioned above. Once you run the cells, your output would be displayed. If it matches the expected output, then you should continue with the next part of the exercise, else, you would have to try again. \n",
    "\n",
    "Please read the [Notebook FAQ](https://www.coursera.org/learn/machine-learning/supplement/S9fb6/jupyter-notebook-faq) to get familiar with the Jupyter environment and the commands. We also highly recommend going to your workspace to explore the files you will be working with before starting the exercise.\n",
    "\n",
    "To go to the workspace: press on File ==> Open.\n",
    "\n",
    "### NOTE:\n",
    "You will find cells which contain the comment % GRADED FUNCTION: functionName. Do not edit that comment. Those cells will be used to grade your assignment. Each block of code with that comment should only have the function. \n",
    "\n",
    "#### After submitting your assignment, you can [check your grades here](https://www.coursera.org/learn/machine-learning/programming/V6ZFz/linear-regression-optional). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with multiple variables\n",
    "=========================================\n",
    "\n",
    "You will implement linear regression with multiple\n",
    "variables to predict housing prices\n",
    ". Suppose you are selling your\n",
    "house and you want to know what a good market price would be. One way to\n",
    "do this is to first collect information on recent houses sold and to make a\n",
    "model of housing prices. The file ***[ex1data2.txt](ex1data2.txt)*** contains a\n",
    "training set of housing prices in Portland, Oregon. The first column is\n",
    "the size of the house (in square feet), the second column is the number\n",
    "of bedrooms, and the third column is the price of the house. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Feature Normalization \n",
    "---------\n",
    "\n",
    "By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ\n",
    "by orders of magnitude, first performing feature scaling can make\n",
    "gradient descent converge much more quickly.\n",
    "Your task here is to complete the function, featureNormalize, in the cell below. You should:\n",
    "\n",
    "-   Subtract the mean value of each feature from the dataset.\n",
    "\n",
    "-   After subtracting the mean, additionally scale (divide) the feature\n",
    "    values by their respective “standard deviations.”\n",
    "\n",
    "The standard deviation is a way of measuring how much variation there is\n",
    "in the range of values of a particular feature (most data points will\n",
    "lie within $\\pm$2 standard deviations of the mean); this is an\n",
    "alternative to taking the range of values (max-min). In Octave/MATLAB,\n",
    "you can use the `std` function to compute the standard deviation.\n",
    "For example, the quantity\n",
    "`X(:,1)` contains all the values of $x_1$ (house sizes) in the\n",
    "training set, so `std(X(:,1))` computes the standard deviation of\n",
    "the house sizes. When you call featureNormalize(X),\n",
    "the extra column of 1’s corresponding to $x_0 = 1$ has not yet been\n",
    "added to `X`. You will do\n",
    "this for all the features and your code should work with datasets of all\n",
    "sizes (any number of features / examples). Note that each column of the\n",
    "matrix `X` corresponds to one feature.\n",
    "\n",
    "### Instructions\n",
    "When normalizing the features, it is important\n",
    "to store the values used for normalization - the *mean value* and the\n",
    "*standard deviation*. After learning the\n",
    "parameters from the model, we often want to predict the prices of houses\n",
    "we have not seen before. Given a new $\\mathbf{x}$ value (house size and number of bedrooms), we must first normalize $\\mathbf{x}$ using\n",
    "the mean and standard deviation that we had previously computed from the\n",
    "training set.\n",
    "\n",
    "featureNormalize(X) returns a normalized version of X where the mean value of each feature is 0 and the standard deviation is 1. This is often a good preprocessing step to do when working with learning algorithms.\n",
    "\n",
    "First, for each feature dimension, compute the mean of the feature and subtract it from the dataset,             storing the mean value in mu. Next, compute the standard deviation of each feature and divide each feature by it's standard deviation, storing the standard deviation in sigma. \n",
    "\n",
    "Note that X is a matrix where each column is a feature and each row is an example. You need to perform the normalization separately for each feature. \n",
    "\n",
    "***Hint:*** You might find the 'mean', 'std', and 'bsxfun' functions useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "% Load the Data\n",
    "warning('off'); addpath('../readonly/Assignment1b/');\n",
    "data = load('ex1data2.txt');\n",
    "X = data(:, 1:2);\n",
    "y = data(:, 3);\n",
    "m = length(y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: featureNormalize\n",
    "function [X_norm, mu, sigma] = featureNormalize(X)\n",
    "\n",
    "% You need to set these values correctly\n",
    "X_norm = X;\n",
    "mu = zeros(1, size(X, 2));\n",
    "sigma = zeros(1, size(X, 2));\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "mu = mean(X);\n",
    "sigma = std(X);\n",
    "# X_norm = bsxfun(@rdivide, (X - mu), sigma);\n",
    "X_norm = (X - mu) ./ std(X);\n",
    "% ============================================================\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =\n",
      "\n",
      "   1.3001e-01  -2.2368e-01\n",
      "  -5.0419e-01  -2.2368e-01\n",
      "   5.0248e-01  -2.2368e-01\n",
      "  -7.3572e-01  -1.5378e+00\n",
      "   1.2575e+00   1.0904e+00\n",
      "  -1.9732e-02   1.0904e+00\n",
      "  -5.8724e-01  -2.2368e-01\n",
      "  -7.2188e-01  -2.2368e-01\n",
      "  -7.8102e-01  -2.2368e-01\n",
      "  -6.3757e-01  -2.2368e-01\n",
      "  -7.6357e-02   1.0904e+00\n",
      "  -8.5674e-04  -2.2368e-01\n",
      "  -1.3927e-01  -2.2368e-01\n",
      "   3.1173e+00   2.4045e+00\n",
      "  -9.2196e-01  -2.2368e-01\n",
      "   3.7664e-01   1.0904e+00\n",
      "  -8.5652e-01  -1.5378e+00\n",
      "  -9.6222e-01  -2.2368e-01\n",
      "   7.6547e-01   1.0904e+00\n",
      "   1.2965e+00   1.0904e+00\n",
      "  -2.9405e-01  -2.2368e-01\n",
      "  -1.4179e-01  -1.5378e+00\n",
      "  -4.9916e-01  -2.2368e-01\n",
      "  -4.8673e-02   1.0904e+00\n",
      "   2.3774e+00  -2.2368e-01\n",
      "  -1.1334e+00  -2.2368e-01\n",
      "  -6.8287e-01  -2.2368e-01\n",
      "   6.6103e-01  -2.2368e-01\n",
      "   2.5081e-01  -2.2368e-01\n",
      "   8.0070e-01  -2.2368e-01\n",
      "  -2.0345e-01  -1.5378e+00\n",
      "  -1.2592e+00  -2.8519e+00\n",
      "   4.9477e-02   1.0904e+00\n",
      "   1.4299e+00  -2.2368e-01\n",
      "  -2.3868e-01   1.0904e+00\n",
      "  -7.0930e-01  -2.2368e-01\n",
      "  -9.5845e-01  -2.2368e-01\n",
      "   1.6524e-01   1.0904e+00\n",
      "   2.7864e+00   1.0904e+00\n",
      "   2.0299e-01   1.0904e+00\n",
      "  -4.2366e-01  -1.5378e+00\n",
      "   2.9863e-01  -2.2368e-01\n",
      "   7.1262e-01   1.0904e+00\n",
      "  -1.0075e+00  -2.2368e-01\n",
      "  -1.4454e+00  -1.5378e+00\n",
      "  -1.8709e-01   1.0904e+00\n",
      "  -1.0037e+00  -2.2368e-01\n",
      "\n",
      "mu =\n",
      "\n",
      "   2000.6809      3.1702\n",
      "\n",
      "sigma =\n",
      "\n",
      "   794.70235     0.76098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% Scale features and set them to zero mean\n",
    "[X mu sigma] = featureNormalize(X)\n",
    "\n",
    "% Add intercept term to X\n",
    "X = [ones(m, 1) X];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Cost for Multiple Variables: \n",
    "\n",
    "\n",
    "Previously, you implemented gradient descent on a univariate regression\n",
    "problem. The only difference now is that there is one more feature in\n",
    "the matrix `X`. The hypothesis function and the batch gradient\n",
    "descent update rule remain unchanged. You first have to implement \n",
    "the cost function for multiple variables. Once you are done implementing it, you will complete the gradient descent for linear regression with\n",
    "multiple variables. If your code in the previous exercise (single variable)\n",
    "already supports multiple variables, you can use it here too. Make sure\n",
    "your code supports any number of features and is well-vectorized. You\n",
    "can use `size(X, 2)` to find out how many features are present in\n",
    "the dataset.\n",
    "\n",
    "\n",
    "In the multivariate case, the cost function can also be written in the\n",
    "following vectorized form: \n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m}(X\\theta-\\vec{y})^{T}(X\\theta-\\vec{y})$$\n",
    "\n",
    "where \n",
    "\n",
    "$$X=\n",
    "\\left(\\begin{array}{cc} \n",
    "---(x^{(1)})^T ---\\\\\n",
    "---(x^{(2)})^T ---\\\\\n",
    "       .          \\\\\n",
    "       .           \\\\\n",
    "---(x^{(m)})^T ---\\\\\n",
    "\\end{array}\\right) \\; \\; \\; \\;  \\vec{y} = \n",
    "\\left(\\begin{array}{cc} \n",
    "y^{(1)}\\\\ \n",
    "y^{(2)}\\\\ \n",
    "  .    \\\\\n",
    "  .  \\\\\n",
    "y^{(m)}\\\\ \n",
    "\\end{array}\\right) $$\n",
    "\n",
    "The vectorized version is efficient when you’re working with numerical\n",
    "computing tools like Octave/MATLAB. If you are an expert with matrix\n",
    "operations, you can prove to yourself that the two forms are equivalent.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "The function below should compute the cost for linear regression with multiple variables.\n",
    "\n",
    "J = computeCostMulti(X, y, theta) computes the cost of using theta as the parameter for linear regression to fit the data points in X and y.\n",
    "\n",
    "Complete the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: computeCostMulti\n",
    "function J = computeCostMulti(X, y, theta)\n",
    "\n",
    "m = length(y); % number of training examples\n",
    "J = 0;         % compute the cost and set it to J\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "J = 1/(2*m) * (X * theta - y)' * (X * theta - y);\n",
    "\n",
    "% ============================================================\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans =    6.5592e+10\n"
     ]
    }
   ],
   "source": [
    "% Load data\n",
    "data = load('ex1data2.txt'); X = data(:, 1:2); y = data(:, 3); m = length(y);\n",
    "\n",
    "% Normalize and add ones\n",
    "[X mu sigma] = featureNormalize(X);\n",
    "X = [ones(m, 1) X];\n",
    "\n",
    "% Compute cost for theta with all zeros\n",
    "theta = zeros(3, 1);         % We already loaded X and y\n",
    "\n",
    "computeCostMulti(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "6.5592e+10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradient Descent for Multiple Variables\n",
    "\n",
    "gradientDescentMulti uses gradient descent to learn theta.\n",
    "\n",
    "theta = gradientDescentMulti(x, y, theta, alpha, num_iters) updates theta by taking num_iters gradient steps with learning rate alpha. You should perform a single gradient step on the parameter vector theta.\n",
    "\n",
    "Remember that this is the equation you should be using to implement the gradient descent: \n",
    "$$\\theta_j := \n",
    "\\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)} \\;\\;\\;\\;\\;\\mbox{(simultaneously update $\\theta_j$ for all $j$)} $$\n",
    "\n",
    "You could compute the gradient first:\n",
    "\n",
    "$$\\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
    "and then update theta.\n",
    "\n",
    "***Hint***: While debugging, it can be useful to print out the values of the cost function (computeCostMulti) and gradient here. Once you are done you could just press on Cell ==> Run All to make sure that you have initialized all the variables in the correct order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables in the current scope:\n",
      "\n",
      "   Attr Name           Size                     Bytes  Class\n",
      "   ==== ====           ====                     =====  ===== \n",
      "        X             47x3                       1128  double\n",
      "        alpha          1x1                          8  double\n",
      "        ans            1x1                          8  double\n",
      "        data          47x3                       1128  double\n",
      "        m              1x1                          8  double\n",
      "        mu             1x2                         16  double\n",
      "        num_iters      1x1                          8  double\n",
      "        sigma          1x2                         16  double\n",
      "        theta          3x1                         24  double\n",
      "        y             47x1                        376  double\n",
      "\n",
      "Total is 340 elements using 2720 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% Load variables\n",
    "alpha = 0.01;                % Initializing alpha\n",
    "num_iters = 400;             % Number of iterations \n",
    "whos                         % The list of variables should include X, y, theta, alpha and num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: gradientDescentMulti\n",
    "function [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters)\n",
    "\n",
    "m = length(y);                    % number of training examples\n",
    "J_history = zeros(num_iters, 1);  % vector to store the cost at every iteration\n",
    "\n",
    "    for iter = 1:num_iters\n",
    "\n",
    "    % ====================== YOUR CODE HERE =====================\n",
    "\n",
    "    theta -= alpha/m * X' * (X*theta - y);\n",
    "\n",
    "\n",
    "\n",
    "    % ===========================================================\n",
    "    J_history(iter) = computeCostMulti(X, y, theta); % Save the cost J in every iteration    \n",
    "\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have completed the computeCostMulti and gradientDescentMulti functions, you could run the cell below to see a plot of the number of iterations versus the cost. You should modify the value of the price below to predict the price of a new house that has a size of 1650 sq-ft and 3 bedrooms. Recall that the first column of X is all-ones. Thus, it does not need to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =\n",
      "\n",
      "   3.3430e+05\n",
      "   9.9411e+04\n",
      "   3.2670e+03\n",
      "\n",
      "Predicted price of a 1650 sq-ft, 3 br house (using gradient descent):\n",
      " $289221.547371\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAIAAAAP3aGbAAAJNmlDQ1BkZWZhdWx0X3JnYi5pY2MA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\nwAIQjf8H5E8pDSMQF5cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% Initialize some variables\n",
    "[theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters); % run your gradient descent\n",
    "\n",
    "% Plot the convergence graph\n",
    "figure('Position',[0,0,400,400]);\n",
    "plot(1:numel(J_history), J_history, '-b', 'LineWidth', 2);\n",
    "xlabel('Number of iterations');\n",
    "ylabel('Cost J');\n",
    "\n",
    "theta      % Display result from gradient descent\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "x = bsxfun(@rdivide, ([1650 3] - mu), sigma);\n",
    "price = [1 x] * theta;\n",
    "\n",
    "% ============================================================\n",
    "\n",
    "fprintf(['Predicted price of a 1650 sq-ft, 3 br house ' ...\n",
    "         '(using gradient descent):\\n $%f\\n'], price);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional (ungraded) exercise: Selecting learning rates\n",
    "\n",
    "In this part of the exercise, you will get to try out different learning\n",
    "rates for the dataset and find a learning rate that converges quickly.\n",
    "You can change the learning rate by \n",
    "changing the part of the code that sets the learning rate. The next\n",
    "phase will call your gradientDescent function and run gradient descent for about 50 iterations at the chosen\n",
    "learning rate. The function should also return the history of\n",
    "$J(\\theta)$ values in a vector J. After the last iteration, the\n",
    "cell will plot the J values against the number\n",
    "of iterations. If you picked a learning rate within a good range,\n",
    "your plot will look similar to: \n",
    "<img src=\"../readonly/Assignment1b/figure 1.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "\n",
    "If your graph looks\n",
    "very different, especially if your value of $J(\\theta)$ increases or\n",
    "even blows up, adjust your learning rate and try again. We recommend\n",
    "trying values of the learning rate $\\alpha$ on a log-scale, at\n",
    "multiplicative steps of about 3 times the previous value (i.e., 0.3,\n",
    "0.1, 0.03, 0.01 and so on). You may also want to adjust the number of\n",
    "iterations you are running if that will help you see the overall trend\n",
    "in the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation Note**: If your learning rate is too large, $J(\\theta)$ can diverge and \n",
    "`blow up`, resulting in values which are too large for computer calculations. In these situations,\n",
    "Octave/MATLAB will tend to return NaNs. NaN stands for not a number and is often caused by undefined\n",
    "operations that involve $-\\infty$ and $+\\infty$\n",
    "\n",
    "**Tip**: To compare how different learning rates affect\n",
    "convergence, it’s helpful to plot J for several learning rates on the\n",
    "same figure. In Octave/MATLAB, this can be done by performing gradient\n",
    "descent multiple times with a ’hold on’ command between plots.\n",
    "Concretely, if you’ve tried three different values of alpha (you should\n",
    "probably try more values than this) and stored the costs in J1, J2, and\n",
    "J3 you can plot them using the same code in the second half of the cell below. We have done it for you for one example, you should do it with different alpha values.  \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAIAAAAP3aGbAAAJNmlDQ1BkZWZhdWx0X3JnYi5pY2MA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\nwxU4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% Do it for different alpha values. (i.e. initiialize different alphas)\n",
    "alpha_1 = 0.02;\n",
    "num_iters = 400;\n",
    "\n",
    "\n",
    "% Add some code below to run gradientDescentMulit on different alphas and thetas\n",
    "% You could initialize J_2, J_3, etc.. the same way we have have J_1 \n",
    "theta_1 = zeros(3, 1);\n",
    "theta_2 = zeros(3, 1);\n",
    "theta_3 = zeros(3, 1);\n",
    "[theta_1, J_1] = gradientDescentMulti(X, y, theta_1, alpha_1, num_iters);\n",
    "[theta_2, J_2] = gradientDescentMulti(X, y, theta_2, 0.0006, num_iters);\n",
    "[theta_3, J_3] = gradientDescentMulti(X, y, theta_3, 0.09, num_iters);\n",
    "% ----------------------------------------------------------\n",
    "% Plot the convergence graphs\n",
    "figure('Position',[0,0,400,400]);\n",
    "plot(1:numel(J_history), J_history, '-b', 'LineWidth', 2);\n",
    "xlabel('Number of iterations');\n",
    "ylabel('Cost J');\n",
    "\n",
    "% To compare how different learning learning rates affect convergence, \n",
    "% it's helpful to plot J for several learning rates on the same figure.\n",
    "\n",
    "hold on;\n",
    "plot(1:numel(J_1), J_1, 'r', 'LineWidth',2);\n",
    "hold on;\n",
    "plot(1:numel(J_2), J_2(1:400), 'g');\n",
    "plot(1:numel(J_3), J_3(1:400), 'k');\n",
    "\n",
    "% The final arguments 'b', 'r', and 'k' specify different colors for the plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the changes in the convergence curves as the learning rate changes. With a small learning rate, you should find that gradient\n",
    "descent takes a very long time to converge to the optimal value.\n",
    "Conversely, with a large learning rate, gradient descent might not\n",
    "converge or might even diverge. Using the best learning rate that you\n",
    "found, rerun your code to run gradient descent until convergence to find\n",
    "the final values of $\\theta$. Next, use this value of $\\theta$ to\n",
    "predict another time the price of a house with 1650 square feet and 3 bedrooms. You\n",
    "will use these values later to check your implementation of the normal\n",
    "equations. Don’t forget to normalize your features when you make this\n",
    "prediction! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equations\n",
    "\n",
    "In the lecture videos, you learned that the closed-form solution to\n",
    "linear regression is $$\\theta=\\left(X^{T}X\\right)^{-1}X^{T}\\vec{y}.$$\n",
    "Using this formula does not require any feature scaling, and you will\n",
    "get an exact solution in one calculation: there is no “loop until\n",
    "convergence” like in gradient descent. Complete the function below to use the formula above to calculate $\\theta$.\n",
    "Remember that while you don’t need to scale your features, we still need\n",
    "to add a column of 1’s to the X matrix to have an intercept term\n",
    "($\\theta_0$). We already added a one's vector to our X. After you have completed the NormalEqn function, run the cell.\n",
    "\n",
    "***Instructions***\n",
    "\n",
    "normalEqn computes the closed-form solution to linear regression using the normal equations.\n",
    "\n",
    "Compute the closed form solution to linear regression and put the result in theta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% GRADED FUNCTION: normalEqn\n",
    "function [theta] = normalEqn(X, y)\n",
    "\n",
    "theta = zeros(size(X, 2), 1);\n",
    "\n",
    "% ====================== YOUR CODE HERE ======================\n",
    "\n",
    "theta = pinv(X'*X) * X' * y;\n",
    "\n",
    "% ============================================================\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =\n",
      "\n",
      "   3.4041e+05\n",
      "   1.0945e+05\n",
      "  -6.5784e+03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data = load('ex1data2.txt'); X = data(:, 1:2); y = data(:, 3);\n",
    "theta = normalEqn(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Optional (ungraded) exercise:* Now, once you have found $\\theta$ using\n",
    "this method, use it to make a price prediction for a 1650-square-foot\n",
    "house with 3 bedrooms. You should find that gives the same predicted\n",
    "price as the value you obtained using the model fit with gradient\n",
    "descent above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price =    2.9308e+05\n"
     ]
    }
   ],
   "source": [
    "% Try it below\n",
    "x2 = bsxfun(@rdivide, ([1650 3] - mu), sigma);\n",
    "price = [1 x2] * theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost =    2.0433e+09\n"
     ]
    }
   ],
   "source": [
    "cost = computeCostMulti(X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta1 =\n",
      "\n",
      "   334300    99411     3267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "theta1 = [3.3430e+05 9.9411e+04 3.2670e+03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost =    2.1055e+09\n"
     ]
    }
   ],
   "source": [
    "cost = computeCostMulti(X, y, theta1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "machine-learning",
   "graded_item_id": "V6ZFz",
   "launcher_item_id": "QixXS"
  },
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
